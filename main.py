import io
import re
import nltk
import pdfplumber
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt

# ìµœì´ˆ 1íšŒë§Œ í•„ìš”
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

st.set_page_config(page_title="ì„œìš¸ì‹œë¯¼ ê²°í˜¼Â·ê°€ì¡± ë³€í™” ë¶„ì„ ë·°ì–´", layout="wide")

# ---------------------------
# ì‚¬ì´ë“œë°” - í‚¤ì›Œë“œ ì„¤ì •
# ---------------------------
st.sidebar.header("ğŸ” ë¶„ì„ ì„¤ì •")
default_keywords = [
    "í˜¼ì¸ìœ¨", "ì´ˆí˜¼", "ì¬í˜¼", "ì´í˜¼", "ì¶œì‚°", "í•©ê³„ì¶œì‚°ìœ¨",
    "1ì¸ ê°€êµ¬", "ë¹„í˜¼", "ë§Œí˜¼", "ë™ê±°", "ê°€ì¡±í˜•íƒœ", "ì¶œìƒ", "ê³ ë ¹í™”"
]
keywords = st.sidebar.text_area(
    "ê´€ì‹¬ í‚¤ì›Œë“œ(ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
    value="\n".join(default_keywords),
    height=200
).splitlines()
keywords = [k.strip() for k in keywords if k.strip()]

context_window = st.sidebar.slider("ë¬¸ë§¥ ìŠ¤ë‹ˆí« ê¸¸ì´(ë¬¸ììˆ˜)", 60, 400, 160, 20)

st.title("ğŸ“Š ì„œìš¸ì‹œë¯¼ì˜ ê²°í˜¼Â·ê°€ì¡± í˜•íƒœ ë³€í™” â€” PDF ë¶„ì„ ë·°ì–´")
st.caption("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ í‚¤ì›Œë“œ ì¹´ìš´íŠ¸, ë¬¸ë§¥ ìŠ¤ë‹ˆí«, ê°„ë‹¨ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.")

uploaded = st.file_uploader("ë¶„ì„í•  PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

def extract_text_from_pdf(file_bytes: bytes) -> str:
    text_chunks = []
    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
        for page in pdf.pages:
            t = page.extract_text(x_tolerance=1.5, y_tolerance=1.5) or ""
            # í˜ì´ì§€ ë²ˆí˜¸ ë§ˆì»¤ ë“± ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
            t = re.sub(r"[ \t]+", " ", t)
            text_chunks.append(t.strip())
    return "\n\n".join(text_chunks)

def split_sentences_rough_korean(text: str):
    # í•œê¸€ ë¬¸ì¥ ë¶„ë¦¬(ê°„ë‹¨ ë²„ì „): ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ ê¸°ì¤€ + NLTK ë³´ì¡°
    # ë³´ê³ ì„œ ë¬¸ì²´ íŠ¹ì„±ìƒ ê°œí–‰ì„ ë¬¸ì¥ ê²½ê³„ë¡œë„ í™œìš©
    text = re.sub(r"([.!?])", r"\1 ", text)
    # NLTK sentence tokenize (ì–¸ì–´ í˜¼ìš© ë¬¸ì„œì—ë„ ë¬´ë‚œ)
    sents = nltk.sent_tokenize(text)
    # ê°œí–‰ ê¸°ì¤€ ë³´ê°•
    more = []
    for s in sents:
        more.extend([ss.strip() for ss in re.split(r"\n+", s) if ss.strip()])
    return [s for s in more if len(s) > 1]

def keyword_hits(sentences, kw):
    # ëŒ€ì†Œë¬¸ì/ë„ì–´ì“°ê¸° ë³€í˜•ì— ê°•ê±´í•˜ê²Œ(í•œê¸€ì€ ê·¸ëŒ€ë¡œ, ì˜ë¬¸ì€ ì†Œë¬¸ì)
    kw_norm = kw.lower()
    rows = []
    for i, s in enumerate(sentences):
        s_norm = s.lower()
        if kw_norm in s_norm:
            rows.append({"keyword": kw, "sentence_idx": i, "sentence": s})
    return rows

def make_snippet(text, hit, window=160):
    s = hit["sentence"]
    # ì›ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì¥ ìœ„ì¹˜ ëª» ì¡ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ë¬¸ì¥ ìì²´ë¡œ ìŠ¤ë‹ˆí« êµ¬ì„±
    if len(s) <= window:
        return s
    mid = len(s) // 2
    return s[: window//2] + " â€¦ " + s[-window//2 :]

if uploaded:
    with st.spinner("PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘â€¦"):
        raw_text = extract_text_from_pdf(uploaded.read())

    # ë¬¸ì¥ ë¶„ë¦¬
    sentences = split_sentences_rough_korean(raw_text)
    total_sentences = len(sentences)

    st.success(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: ì•½ {total_sentences}ê°œ ë¬¸ì¥")

    # ---------------------------
    # í‚¤ì›Œë“œ ë§¤ì¹­
    # ---------------------------
    hits_all = []
    for kw in keywords:
        hits = keyword_hits(sentences, kw)
        hits_all.extend(hits)

    if hits_all:
        df_hits = pd.DataFrame(hits_all)
        # í‚¤ì›Œë“œë³„ ì¹´ìš´íŠ¸
        counts = df_hits["keyword"].value_counts().rename_axis("keyword").reset_index(name="count")

        # ì‹œê°í™”
        st.subheader("ğŸ“ˆ í‚¤ì›Œë“œ ì¶œí˜„ ë¹ˆë„")
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.bar(counts["keyword"], counts["count"])
        ax.set_ylabel("ë¹ˆë„")
        ax.set_xlabel("í‚¤ì›Œë“œ")
        ax.set_xticklabels(counts["keyword"], rotation=20, ha="right")
        st.pyplot(fig, use_container_width=True)

        # ìŠ¤ë‹ˆí« í…Œì´ë¸”
        st.subheader("ğŸ§© ë¬¸ë§¥ ìŠ¤ë‹ˆí«")
        df_hits["snippet"] = df_hits.apply(lambda r: make_snippet(raw_text, r, context_window), axis=1)
        show_cols = ["keyword", "sentence_idx", "snippet"]
        st.dataframe(df_hits[show_cols], use_container_width=True, height=360)

        # ë‹¤ìš´ë¡œë“œ
        csv = df_hits[show_cols + ["sentence"]].to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "CSVë¡œ ë‚´ë³´ë‚´ê¸°",
            data=csv,
            file_name="keyword_snippets.csv",
            mime="text/csv"
        )
    else:
        st.info("ì„¤ì •í•œ í‚¤ì›Œë“œê°€ ë³¸ë¬¸ì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ í‚¤ì›Œë“œë¥¼ ë°”ê¿”ë³´ì„¸ìš”.")

    # ---------------------------
    # ê°„ë‹¨ ìš”ì•½(ë£° ê¸°ë°˜)
    # ---------------------------
    st.subheader("ğŸ“ ê°„ë‹¨ ìš”ì•½(ë£° ê¸°ë°˜)")
    # ì—°ë„/ì§€í‘œ ì¶”ì¶œ ë°ëª¨: '2010', '2024' ê°™ì€ 4ìë¦¬ ì—°ë„
    years = sorted(set(re.findall(r"\b(19\d{2}|20\d{2})\b", raw_text)))
    bullets = []

    if years:
        bullets.append(f"- ë³´ê³ ì„œ ë‚´ í…ìŠ¤íŠ¸ì—ì„œ ì‹ë³„ëœ ì—°ë„ ë²”ìœ„: **{years[0]}â€“{years[-1]}**")

    # ì£¼ìš” ìš©ì–´ ì¡´ì¬ ì—¬ë¶€
    for term in ["í˜¼ì¸ìœ¨", "ì´ˆí˜¼", "ì´í˜¼", "ì¶œì‚°", "í•©ê³„ì¶œì‚°ìœ¨", "1ì¸ ê°€êµ¬", "ë¹„í˜¼", "ë™ê±°", "ë§Œí˜¼"]:
        if re.search(term, raw_text):
            bullets.append(f"- **{term}** ê´€ë ¨ ì„œìˆ ì´ í¬í•¨ë˜ì–´ ìˆìŒ")

    if not bullets:
        bullets.append("- ê·œì¹™ ê¸°ë°˜ ìš”ì•½ í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ/ìŠ¤ë‹ˆí«ì—ì„œ ì§ì ‘ í™•ì¸í•˜ì„¸ìš”.")

    for b in bullets:
        st.markdown(b)

    # ---------------------------
    # ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°
    # ---------------------------
    with st.expander("ğŸ“„ ë³¸ë¬¸ ì›ë¬¸ ì¼ë¶€ ë³´ê¸°"):
        st.text_area("í…ìŠ¤íŠ¸(ì¼ë¶€)", value=raw_text[:8000], height=260)
else:
    st.info("ì™¼ìª½ ìƒë‹¨ì—ì„œ PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
    st.markdown("""
- ê¸°ë³¸ í‚¤ì›Œë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜, **ì‚¬ì´ë“œë°”ì—ì„œ ê´€ì‹¬ í‚¤ì›Œë“œ**ë¥¼ ë°”ê¿”ë³´ì„¸ìš”.  
- ì—…ë¡œë“œ í›„ **í‚¤ì›Œë“œ ë¹ˆë„ ë§‰ëŒ€ê·¸ë˜í”„**, **ë¬¸ë§¥ ìŠ¤ë‹ˆí« í…Œì´ë¸”**, **ê°„ë‹¨ ìš”ì•½**ì´ ìë™ ìƒì„±ë©ë‹ˆë‹¤.
""")
