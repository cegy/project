import io
import os
import re
import pdfplumber
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt

st.set_page_config(page_title="ì„œìš¸ì‹œë¯¼ ê²°í˜¼Â·ê°€ì¡± ë³€í™” ë¶„ì„ (ë¡œì»¬ PDF)", layout="wide")

# ---------------------------
# ê³ ì • íŒŒì¼ ê²½ë¡œ (main.pyì™€ ë™ì¼ í´ë”)
# ---------------------------
PDF_FILENAME = "ì„œìš¸ì‹œë¯¼ì˜+ê²°í˜¼ê³¼+ê°€ì¡±+í˜•íƒœì˜+ë³€í™”+ë¶„ì„.pdf"
PDF_PATH = os.path.join(os.path.dirname(__file__), PDF_FILENAME)

# ---------------------------
# ìºì‹œ ìœ í‹¸
# ---------------------------
@st.cache_data(show_spinner=False)
def read_pdf_bytes(path: str) -> bytes:
    with open(path, "rb") as f:
        return f.read()

@st.cache_data(show_spinner=False)
def extract_text_from_pdf(file_bytes: bytes) -> str:
    text_chunks = []
    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
        for page in pdf.pages:
            t = page.extract_text(x_tolerance=1.5, y_tolerance=1.5) or ""
            t = re.sub(r"[ \t]+", " ", t)
            text_chunks.append(t.strip())
    return "\n\n".join(text_chunks)

def split_sentences_rough_korean(text: str):
    # ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ/ê°œí–‰ ê¸°ì¤€ ê°„ë‹¨ ë¶„ë¦¬ (NLTK ë¯¸ì‚¬ìš©)
    sents = re.split(r'(?<=[.!?])\s+|\n+', text)
    return [s.strip() for s in sents if len(s.strip()) > 1]

def keyword_hits(sentences, kw):
    kw_norm = kw.lower()
    rows = []
    for i, s in enumerate(sentences):
        if kw_norm in s.lower():
            rows.append({"keyword": kw, "sentence_idx": i, "sentence": s})
    return rows

def make_snippet(hit, window=160):
    s = hit["sentence"]
    if len(s) <= window:
        return s
    return s[: window//2] + " â€¦ " + s[-window//2 :]

# ---------------------------
# ì‚¬ì´ë“œë°” - ë¶„ì„ ì„¤ì •
# ---------------------------
st.sidebar.header("ğŸ” ë¶„ì„ ì„¤ì •")
default_keywords = [
    "í˜¼ì¸ìœ¨", "ì´ˆí˜¼", "ì¬í˜¼", "ì´í˜¼", "ì¶œì‚°", "í•©ê³„ì¶œì‚°ìœ¨",
    "1ì¸ ê°€êµ¬", "ë¹„í˜¼", "ë§Œí˜¼", "ë™ê±°", "ê°€ì¡±í˜•íƒœ", "ì¶œìƒ", "ê³ ë ¹í™”"
]
keywords = st.sidebar.text_area(
    "ê´€ì‹¬ í‚¤ì›Œë“œ(ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
    value="\n".join(default_keywords),
    height=200
).splitlines()
keywords = [k.strip() for k in keywords if k.strip()]
context_window = st.sidebar.slider("ë¬¸ë§¥ ìŠ¤ë‹ˆí« ê¸¸ì´(ë¬¸ììˆ˜)", 60, 400, 160, 20)

# ---------------------------
# ìƒë‹¨ ì •ë³´
# ---------------------------
st.title("ğŸ“Š ì„œìš¸ì‹œë¯¼ì˜ ê²°í˜¼Â·ê°€ì¡± í˜•íƒœ ë³€í™” â€” ë¡œì»¬ PDF ë¶„ì„ ë·°ì–´")
st.caption("ê°™ì€ í´ë”ì˜ PDFë¥¼ ì§ì ‘ ì½ì–´ í‚¤ì›Œë“œ ë¹ˆë„, ë¬¸ë§¥ ìŠ¤ë‹ˆí«, ê°„ë‹¨ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.")
st.write(f"ëŒ€ìƒ íŒŒì¼: **{PDF_FILENAME}**")
st.caption(f"ê²½ë¡œ: `{PDF_PATH}`")

# ---------------------------
# íŒŒì¼ ìœ íš¨ì„± ì²´í¬
# ---------------------------
if not os.path.exists(PDF_PATH):
    st.error("ì§€ì •ëœ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `main.py`ì™€ ê°™ì€ í´ë”ì— "
             f"`{PDF_FILENAME}` íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# ---------------------------
# ë³¸ë¬¸ ì²˜ë¦¬
# ---------------------------
with st.spinner("PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘â€¦"):
    raw_bytes = read_pdf_bytes(PDF_PATH)
    raw_text = extract_text_from_pdf(raw_bytes)

sentences = split_sentences_rough_korean(raw_text)
st.success(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: ì•½ {len(sentences)}ê°œ ë¬¸ì¥")

# ---------------------------
# í‚¤ì›Œë“œ ê²€ìƒ‰/ì‹œê°í™”
# ---------------------------
hits_all = []
for kw in keywords:
    hits_all.extend(keyword_hits(sentences, kw))

if hits_all:
    df_hits = pd.DataFrame(hits_all)
    counts = df_hits["keyword"].value_counts().rename_axis("keyword").reset_index(name="count")

    st.subheader("ğŸ“ˆ í‚¤ì›Œë“œ ì¶œí˜„ ë¹ˆë„")
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.bar(counts["keyword"], counts["count"])
    ax.set_ylabel("ë¹ˆë„")
    ax.set_xlabel("í‚¤ì›Œë“œ")
    ax.set_xticklabels(counts["keyword"], rotation=20, ha="right")
    st.pyplot(fig, use_container_width=True)

    st.subheader("ğŸ§© ë¬¸ë§¥ ìŠ¤ë‹ˆí«")
    df_hits["snippet"] = df_hits.apply(lambda r: make_snippet(r, context_window), axis=1)
    show_cols = ["keyword", "sentence_idx", "snippet"]
    st.dataframe(df_hits[show_cols], use_container_width=True, height=360)

    csv = df_hits[show_cols + ["sentence"]].to_csv(index=False).encode("utf-8-sig")
    st.download_button("CSVë¡œ ë‚´ë³´ë‚´ê¸°", data=csv, file_name="keyword_snippets.csv", mime="text/csv")
else:
    st.info("ì„¤ì •í•œ í‚¤ì›Œë“œê°€ ë³¸ë¬¸ì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¡°ì •í•´ ë³´ì„¸ìš”.")

# ---------------------------
# ê°„ë‹¨ ìš”ì•½(ë£° ê¸°ë°˜)
# ---------------------------
st.subheader("ğŸ“ ê°„ë‹¨ ìš”ì•½(ë£° ê¸°ë°˜)")
years = sorted(set(re.findall(r"\b(19\d{2}|20\d{2})\b", raw_text)))
bullets = []
if years:
    bullets.append(f"- ë³´ê³ ì„œì— ë“±ì¥í•˜ëŠ” ì—°ë„ ë²”ìœ„: **{years[0]}â€“{years[-1]}**")
for term in ["í˜¼ì¸ìœ¨", "ì´ˆí˜¼", "ì´í˜¼", "ì¶œì‚°", "í•©ê³„ì¶œì‚°ìœ¨", "1ì¸ ê°€êµ¬", "ë¹„í˜¼", "ë™ê±°", "ë§Œí˜¼"]:
    if re.search(term, raw_text):
        bullets.append(f"- **{term}** ê´€ë ¨ ì„œìˆ ì´ í¬í•¨ë˜ì–´ ìˆìŒ")
if not bullets:
    bullets.append("- ì£¼ìš” ìš©ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ/ìŠ¤ë‹ˆí« ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.")
for b in bullets:
    st.markdown(b)

# ---------------------------
# ì›ë¬¸ ì¼ë¶€ ë¯¸ë¦¬ë³´ê¸°
# ---------------------------
with st.expander("ğŸ“„ ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°"):
    st.text_area("í…ìŠ¤íŠ¸(ì¼ë¶€)", value=raw_text[:8000], height=260)
